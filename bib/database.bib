% Encoding: UTF-8

@book{Brunton_book_2019,
  place     = {Cambridge},
  title     = {Data-Driven Science and Engineering: Machine Learning, Dynamical Systems, and Control},
  publisher = {Cambridge University Press},
  author    = {Brunton, Steven L. and Kutz, J. Nathan},
  year      = {2019}
},
@article{van_hasselt_deep_2016,
  title    = {Deep {Reinforcement} {Learning} with {Double} {Q}-{Learning}},
  volume   = {30},
  issn     = {2374-3468, 2159-5399},
  url      = {https://ojs.aaai.org/index.php/AAAI/article/view/10295},
  doi      = {10.1609/aaai.v30i1.10295},
  abstract = {The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented.  In this paper, we answer all these questions affirmatively.  In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain.  We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation.  We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.},
  number   = {1},
  urldate  = {2024-05-01},
  journal  = {Proceedings of the AAAI Conference on Artificial Intelligence},
  author   = {Van Hasselt, Hado and Guez, Arthur and Silver, David},
  month    = mar,
  year     = {2016}
}

@incollection{sanghi_introduction_2021,
  address   = {Berkeley, CA},
  title     = {Introduction to {Reinforcement} {Learning}},
  isbn      = {978-1-4842-6809-4},
  url       = {https://doi.org/10.1007/978-1-4842-6809-4_1},
  abstract  = {Reinforcement learning is one of the fastest growing disciplines and is helping to make AI real. Combining deep learning with reinforcement learning has led to many significant advances that are increasingly getting machines closer to acting the way humans do. In this book, we will start with the basics and finish up with mastering some of the most recent developments in the field. There will be a good mix of theory (with minimal mathematics) and code implementations using PyTorch as well as TensorFlow.},
  language  = {en},
  urldate   = {2024-05-01},
  booktitle = {Deep {Reinforcement} {Learning} with {Python}: {With} {PyTorch}, {TensorFlow} and {OpenAI} {Gym}},
  publisher = {Apress},
  author    = {Sanghi, Nimish},
  editor    = {Sanghi, Nimish},
  year      = {2021},
  doi       = {10.1007/978-1-4842-6809-4_1},
  pages     = {1--17}
},
@article{mnih_human-level_2015,
  title    = {Human-level control through deep reinforcement learning},
  volume   = {518},
  issn     = {0028-0836, 1476-4687},
  url      = {https://www.nature.com/articles/nature14236},
  doi      = {10.1038/nature14236},
  language = {en},
  number   = {7540},
  urldate  = {2024-05-01},
  journal  = {Nature},
  author   = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  month    = feb,
  year     = {2015},
  pages    = {529--533}
},
@article{vinyals_grandmaster_2019,
  title     = {Grandmaster level in {StarCraft} {II} using multi-agent reinforcement learning},
  volume    = {575},
  copyright = {2019 The Author(s), under exclusive licence to Springer Nature Limited},
  issn      = {1476-4687},
  url       = {https://www.nature.com/articles/s41586-019-1724-z},
  doi       = {10.1038/s41586-019-1724-z},
  abstract  = {Many real-world applications require artificial agents to compete and coordinate with other agents in complex environments. As a stepping stone to this goal, the domain of StarCraft has emerged as an important challenge for artificial intelligence research, owing to its iconic and enduring status among the most difficult professional esports and its relevance to the real world in terms of its raw complexity and multi-agent challenges. Over the course of a decade and numerous competitions1–3, the strongest agents have simplified important aspects of the game, utilized superhuman capabilities, or employed hand-crafted sub-systems4. Despite these advantages, no previous agent has come close to matching the overall skill of top StarCraft players. We chose to address the challenge of StarCraft using general-purpose learning methods that are in principle applicable to other complex domains: a multi-agent reinforcement learning algorithm that uses data from both human and agent games within a diverse league of continually adapting strategies and counter-strategies, each represented by deep neural networks5,6. We evaluated our agent, AlphaStar, in the full game of StarCraft II, through a series of online games against human players. AlphaStar was rated at Grandmaster level for all three StarCraft races and above 99.8\% of officially ranked human players.},
  language  = {en},
  number    = {7782},
  urldate   = {2024-05-01},
  journal   = {Nature},
  author    = {Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M. and Mathieu, Michaël and Dudzik, Andrew and Chung, Junyoung and Choi, David H. and Powell, Richard and Ewalds, Timo and Georgiev, Petko and Oh, Junhyuk and Horgan, Dan and Kroiss, Manuel and Danihelka, Ivo and Huang, Aja and Sifre, Laurent and Cai, Trevor and Agapiou, John P. and Jaderberg, Max and Vezhnevets, Alexander S. and Leblond, Rémi and Pohlen, Tobias and Dalibard, Valentin and Budden, David and Sulsky, Yury and Molloy, James and Paine, Tom L. and Gulcehre, Caglar and Wang, Ziyu and Pfaff, Tobias and Wu, Yuhuai and Ring, Roman and Yogatama, Dani and Wünsch, Dario and McKinney, Katrina and Smith, Oliver and Schaul, Tom and Lillicrap, Timothy and Kavukcuoglu, Koray and Hassabis, Demis and Apps, Chris and Silver, David},
  month     = nov,
  year      = {2019},
  note      = {Publisher: Nature Publishing Group},
  keywords  = {Computer science, Statistics},
  pages     = {350--354},
  file      = {Full Text PDF:C\:\\Users\\xwx\\Zotero\\storage\\F7F8EUGR\\Vinyals 等 - 2019 - Grandmaster level in StarCraft II using multi-agen.pdf:application/pdf}
},
@article{silver_mastering_2016,
  title    = {Mastering the game of {Go} with deep neural networks and tree search},
  volume   = {529},
  issn     = {0028-0836, 1476-4687},
  url      = {https://www.nature.com/articles/nature16961},
  doi      = {10.1038/nature16961},
  language = {en},
  number   = {7587},
  urldate  = {2024-05-01},
  journal  = {Nature},
  author   = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
  month    = jan,
  year     = {2016},
  pages    = {484--489}
},
@article{silver_mastering_2017,
  title    = {Mastering the game of {Go} without human knowledge},
  volume   = {550},
  issn     = {0028-0836, 1476-4687},
  url      = {https://www.nature.com/articles/nature24270},
  doi      = {10.1038/nature24270},
  language = {en},
  number   = {7676},
  urldate  = {2024-05-01},
  journal  = {Nature},
  author   = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and Van Den Driessche, George and Graepel, Thore and Hassabis, Demis},
  month    = oct,
  year     = {2017},
  pages    = {354--359}
}

@article{silver_general_2018,
  title    = {A general reinforcement learning algorithm that masters chess, shogi, and {Go} through self-play},
  volume   = {362},
  issn     = {0036-8075, 1095-9203},
  url      = {https://www.science.org/doi/10.1126/science.aar6404},
  doi      = {10.1126/science.aar6404},
  abstract = {One program to rule them all
              
              Computers can beat humans at increasingly complex games, including chess and Go. However, these programs are typically constructed for a particular game, exploiting its properties, such as the symmetries of the board on which it is played. Silver
              et al.
              developed a program called AlphaZero, which taught itself to play Go, chess, and shogi (a Japanese version of chess) (see the Editorial, and the Perspective by Campbell). AlphaZero managed to beat state-of-the-art programs specializing in these three games. The ability of AlphaZero to adapt to various game rules is a notable step toward achieving a general game-playing system.
              
              
              Science
              , this issue p.
              1140
              ; see also pp.
              1087
              and
              1118
              
              , 
              AlphaZero teaches itself to play three different board games and beats state-of-the-art programs in each.
              , 
              The game of chess is the longest-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. By contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go by reinforcement learning from self-play. In this paper, we generalize this approach into a single AlphaZero algorithm that can achieve superhuman performance in many challenging games. Starting from random play and given no domain knowledge except the game rules, AlphaZero convincingly defeated a world champion program in the games of chess and shogi (Japanese chess), as well as Go.},
  language = {en},
  number   = {6419},
  urldate  = {2024-05-01},
  journal  = {Science},
  author   = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
  month    = dec,
  year     = {2018},
  pages    = {1140--1144}
},
@article{popova_deep_2018,
  title    = {Deep reinforcement learning for de novo drug design},
  volume   = {4},
  issn     = {2375-2548},
  url      = {https://www.science.org/doi/10.1126/sciadv.aap7885},
  doi      = {10.1126/sciadv.aap7885},
  abstract = {We introduce an artificial intelligence approach to de novo design of molecules with desired physical or biological properties.
              , 
              We have devised and implemented a novel computational strategy for de novo design of molecules with desired properties termed ReLeaSE (Reinforcement Learning for Structural Evolution). On the basis of deep and reinforcement learning (RL) approaches, ReLeaSE integrates two deep neural networks—generative and predictive—that are trained separately but are used jointly to generate novel targeted chemical libraries. ReLeaSE uses simple representation of molecules by their simplified molecular-input line-entry system (SMILES) strings only. Generative models are trained with a stack-augmented memory network to produce chemically feasible SMILES strings, and predictive models are derived to forecast the desired properties of the de novo–generated compounds. In the first phase of the method, generative and predictive models are trained separately with a supervised learning algorithm. In the second phase, both models are trained jointly with the RL approach to bias the generation of new chemical structures toward those with the desired physical and/or biological properties. In the proof-of-concept study, we have used the ReLeaSE method to design chemical libraries with a bias toward structural complexity or toward compounds with maximal, minimal, or specific range of physical properties, such as melting point or hydrophobicity, or toward compounds with inhibitory activity against Janus protein kinase 2. The approach proposed herein can find a general use for generating targeted chemical libraries of novel compounds optimized for either a single desired property or multiple properties.},
  language = {en},
  number   = {7},
  urldate  = {2024-05-01},
  journal  = {Science Advances},
  author   = {Popova, Mariya and Isayev, Olexandr and Tropsha, Alexander},
  month    = jul,
  year     = {2018},
  pages    = {eaap7885}
},

@article{sallab_deep_2017,
  title    = {Deep {Reinforcement} {Learning} framework for {Autonomous} {Driving}},
  volume   = {29},
  issn     = {2470-1173},
  url      = {https://library.imaging.org/ei/articles/29/19/art00012},
  doi      = {10.2352/ISSN.2470-1173.2017.19.AVM-023},
  abstract = {Reinforcement learning is considered to be a strong AI paradigm which can be used to teach machines through interaction with the environment and learning from their mistakes. Despite its perceived utility, it has not yet been successfully applied in automotive applications. Motivated by the successful demonstrations of learning of Atari games and Go by Google DeepMind, we propose a framework for autonomous driving using deep reinforcement learning. This is of particular relevance as it is difficult to pose autonomous driving as a supervised learning problem due to strong interactions with the environment including other vehicles, pedestrians and roadworks. As it is a relatively new area of research for autonomous driving, we provide a short overview of deep reinforcement learning and then describe our proposed framework. It incorporates Recurrent Neural Networks for information integration, enabling the car to handle partially observable scenarios. It also integrates the recent work on attention models to focus on relevant information, thereby reducing the computational complexity for deployment on embedded hardware. The framework was tested in an open source 3D car racing simulator called TORCS. Our simulation results demonstrate learning of autonomous maneuvering in a scenario of complex road curvatures and simple interaction of other vehicles.},
  language = {en},
  urldate  = {2024-05-01},
  journal  = {Electronic Imaging},
  author   = {Sallab, Ahmad EL and Abdou, Mohammed and Perot, Etienne and Yogamani, Senthil and Abdou, Mohammed and Perot, Etienne and Yogamani, Senthil},
  month    = jan,
  year     = {2017},
  note     = {Publisher: Society for Imaging Science and Technology},
  pages    = {70--76},
  file     = {Full Text PDF:C\:\\Users\\xwx\\Zotero\\storage\\TM4DQ3QB\\Sallab 等 - 2017 - Deep Reinforcement Learning framework for Autonomo.pdf:application/pdf}
},
@inproceedings{gu_deep_2017,
  address   = {Singapore, Singapore},
  title     = {Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates},
  isbn      = {978-1-5090-4633-1},
  url       = {http://ieeexplore.ieee.org/document/7989385/},
  doi       = {10.1109/ICRA.2017.7989385},
  urldate   = {2024-05-01},
  booktitle = {2017 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
  publisher = {IEEE},
  author    = {Gu, Shixiang and Holly, Ethan and Lillicrap, Timothy and Levine, Sergey},
  month     = may,
  year      = {2017},
  pages     = {3389--3396}
},

@article{kaufmann_champion-level_2023,
  title     = {Champion-level drone racing using deep reinforcement learning},
  volume    = {620},
  copyright = {2023 The Author(s)},
  issn      = {1476-4687},
  url       = {https://www.nature.com/articles/s41586-023-06419-4},
  doi       = {10.1038/s41586-023-06419-4},
  abstract  = {First-person view (FPV) drone racing is a televised sport in which professional competitors pilot high-speed aircraft through a 3D circuit. Each pilot sees the environment from the perspective of their drone by means of video streamed from an onboard camera. Reaching the level of professional pilots with an autonomous drone is challenging because the robot needs to fly at its physical limits while estimating its speed and location in the circuit exclusively from onboard sensors1. Here we introduce Swift, an autonomous system that can race physical vehicles at the level of the human world champions. The system combines deep reinforcement learning (RL) in simulation with data collected in the physical world. Swift competed against three human champions, including the world champions of two international leagues, in real-world head-to-head races. Swift won several races against each of the human champions and demonstrated the fastest recorded race time. This work represents a milestone for mobile robotics and machine intelligence2, which may inspire the deployment of hybrid learning-based solutions in other physical systems.},
  language  = {en},
  number    = {7976},
  urldate   = {2024-05-01},
  journal   = {Nature},
  author    = {Kaufmann, Elia and Bauersfeld, Leonard and Loquercio, Antonio and Müller, Matthias and Koltun, Vladlen and Scaramuzza, Davide},
  month     = aug,
  year      = {2023},
  note      = {Publisher: Nature Publishing Group},
  keywords  = {Aerospace engineering, Computer science, Electrical and electronic engineering, Mechanical engineering},
  pages     = {982--987}
},

@article{gazzola_reinforcement_2014,
  title    = {Reinforcement {Learning} and {Wavelet} {Adapted} {Vortex} {Methods} for {Simulations} of {Self}-propelled {Swimmers}},
  volume   = {36},
  issn     = {1064-8275, 1095-7197},
  url      = {http://epubs.siam.org/doi/10.1137/130943078},
  doi      = {10.1137/130943078},
  language = {en},
  number   = {3},
  urldate  = {2024-05-01},
  journal  = {SIAM Journal on Scientific Computing},
  author   = {Gazzola, Mattia and Hejazialhosseini, Babak and Koumoutsakos, Petros},
  month    = jan,
  year     = {2014},
  pages    = {B622--B639}
}

@article{colabrese_flow_2017,
  title     = {Flow {Navigation} by {Smart} {Microswimmers} via {Reinforcement} {Learning}},
  volume    = {118},
  copyright = {http://link.aps.org/licenses/aps-default-license},
  issn      = {0031-9007, 1079-7114},
  url       = {http://link.aps.org/doi/10.1103/PhysRevLett.118.158004},
  doi       = {10.1103/PhysRevLett.118.158004},
  language  = {en},
  number    = {15},
  urldate   = {2024-05-01},
  journal   = {Physical Review Letters},
  author    = {Colabrese, Simona and Gustavsson, Kristian and Celani, Antonio and Biferale, Luca},
  month     = apr,
  year      = {2017},
  pages     = {158004}
},
@article{verma_efficient_2018,
  title    = {Efficient collective swimming by harnessing vortices through deep reinforcement learning},
  volume   = {115},
  issn     = {0027-8424, 1091-6490},
  url      = {https://pnas.org/doi/full/10.1073/pnas.1800923115},
  doi      = {10.1073/pnas.1800923115},
  abstract = {Significance
              Can fish reduce their energy expenditure by schooling? We answer affirmatively this longstanding question by combining state-of-the-art direct numerical simulations of the 3D Navier–Stokes equations with reinforcement learning, using recurrent neural networks with long short-term memory cells to account for the unsteadiness of the flow field. Surprisingly, we find that swimming behind a leader is not always associated with energetic benefits for the follower. In turn, we demonstrate that fish can improve their sustained propulsive efficiency by placing themselves at appropriate locations in the wake of other swimmers and intercepting their wake vortices judiciously. The results show that autonomous, “smart” swimmers may exploit unsteady flow fields to reap substantial energetic benefits and have promising implications for robotic swarms.
              , 
              Fish in schooling formations navigate complex flow fields replete with mechanical energy in the vortex wakes of their companions. Their schooling behavior has been associated with evolutionary advantages including energy savings, yet the underlying physical mechanisms remain unknown. We show that fish can improve their sustained propulsive efficiency by placing themselves in appropriate locations in the wake of other swimmers and intercepting judiciously their shed vortices. This swimming strategy leads to collective energy savings and is revealed through a combination of high-fidelity flow simulations with a deep reinforcement learning (RL) algorithm. The RL algorithm relies on a policy defined by deep, recurrent neural nets, with long–short-term memory cells, that are essential for capturing the unsteadiness of the two-way interactions between the fish and the vortical flow field. Surprisingly, we find that swimming in-line with a leader is not associated with energetic benefits for the follower. Instead, “smart swimmer(s)” place themselves at off-center positions, with respect to the axis of the leader(s) and deform their body to synchronize with the momentum of the oncoming vortices, thus enhancing their swimming efficiency at no cost to the leader(s). The results confirm that fish may harvest energy deposited in vortices and support the conjecture that swimming in formation is energetically advantageous. Moreover, this study demonstrates that deep RL can produce navigation algorithms for complex unsteady and vortical flow fields, with promising implications for energy savings in autonomous robotic swarms.},
  language = {en},
  number   = {23},
  urldate  = {2024-05-01},
  journal  = {Proceedings of the National Academy of Sciences},
  author   = {Verma, Siddhartha and Novati, Guido and Koumoutsakos, Petros},
  month    = jun,
  year     = {2018},
  pages    = {5849--5854}
}

@article{novati_controlled_2019,
  title    = {Controlled gliding and perching through deep-reinforcement-learning},
  volume   = {4},
  issn     = {2469-990X},
  url      = {https://link.aps.org/doi/10.1103/PhysRevFluids.4.093902},
  doi      = {10.1103/PhysRevFluids.4.093902},
  language = {en},
  number   = {9},
  urldate  = {2024-05-01},
  journal  = {Physical Review Fluids},
  author   = {Novati, Guido and Mahadevan, L. and Koumoutsakos, Petros},
  month    = sep,
  year     = {2019},
  pages    = {093902}
}

@article{fan_reinforcement_2020,
  title    = {Reinforcement learning for bluff body active flow control in experiments and simulations},
  volume   = {117},
  issn     = {0027-8424, 1091-6490},
  url      = {https://pnas.org/doi/full/10.1073/pnas.2004939117},
  doi      = {10.1073/pnas.2004939117},
  abstract = {Significance
              
              Reinforcement learning (RL) has been applied effectively in games and robotic manipulation. We demonstrate the effectiveness of RL in experimental fluid mechanics by applying it to reduce the drag of circular cylinders in turbulent flow, a canonical fluid–structure interaction problem. Although physics agnostic, RL managed to reduce the drag by
              
              
              30
              \%
              
              
              or reach another specified optimum point very quickly. Following this discovery, we used high-fidelity simulations to probe the underlying physical mechanisms so that the discovered control techniques can be generalized to other similar flow problems. More broadly, RL-guided active control can lead to efficient exploration of additional flow-control strategies in experimental fluid mechanics, potentially paving the way for accelerating scientific discovery and different designs in flow-related engineering problems.
              
              , 
              We have demonstrated the effectiveness of reinforcement learning (RL) in bluff body flow control problems both in experiments and simulations by automatically discovering active control strategies for drag reduction in turbulent flow. Specifically, we aimed to maximize the power gain efficiency by properly selecting the rotational speed of two small cylinders, located parallel to and downstream of the main cylinder. By properly defining rewards and designing noise reduction techniques, and after an automatic sequence of tens of towing experiments, the RL agent was shown to discover a control strategy that is comparable to the optimal strategy found through lengthy systematically planned control experiments. Subsequently, these results were verified by simulations that enabled us to gain insight into the physical mechanisms of the drag reduction process. While RL has been used effectively previously in idealized computer flow simulation studies, this study demonstrates its effectiveness in experimental fluid mechanics and verifies it by simulations, potentially paving the way for efficient exploration of additional active flow control strategies in other complex fluid mechanics applications.},
  language = {en},
  number   = {42},
  urldate  = {2024-05-01},
  journal  = {Proceedings of the National Academy of Sciences},
  author   = {Fan, Dixia and Yang, Liu and Wang, Zhicheng and Triantafyllou, Michael S. and Karniadakis, George Em},
  month    = oct,
  year     = {2020},
  pages    = {26091--26098}
},
@article{degrave_magnetic_2022,
  title    = {Magnetic control of tokamak plasmas through deep reinforcement learning},
  volume   = {602},
  issn     = {0028-0836, 1476-4687},
  url      = {https://www.nature.com/articles/s41586-021-04301-9},
  doi      = {10.1038/s41586-021-04301-9},
  abstract = {Abstract
              
              Nuclear fusion using magnetic confinement, in particular in the tokamak configuration, is a promising path towards sustainable energy. A core challenge is to shape and maintain a high-temperature plasma within the tokamak vessel. This requires high-dimensional, high-frequency, closed-loop control using magnetic actuator coils, further complicated by the diverse requirements across a wide range of plasma configurations. In this work, we introduce a previously undescribed architecture for tokamak magnetic controller design that autonomously learns to command the full set of control coils. This architecture meets control objectives specified at a high level, at the same time satisfying physical and operational constraints. This approach has unprecedented flexibility and generality in problem specification and yields a notable reduction in design effort to produce new plasma configurations. We successfully produce and control a diverse set of plasma configurations on the Tokamak à Configuration Variable
              1,2
              , including elongated, conventional shapes, as well as advanced configurations, such as negative triangularity and ‘snowflake’ configurations. Our approach achieves accurate tracking of the location, current and shape for these configurations. We also demonstrate sustained ‘droplets’ on TCV, in which two separate plasmas are maintained simultaneously within the vessel. This represents a notable advance for tokamak feedback control, showing the potential of reinforcement learning to accelerate research in the fusion domain, and is one of the most challenging real-world systems to which reinforcement learning has been applied.},
  language = {en},
  number   = {7897},
  urldate  = {2024-05-01},
  journal  = {Nature},
  author   = {Degrave, Jonas and Felici, Federico and Buchli, Jonas and Neunert, Michael and Tracey, Brendan and Carpanese, Francesco and Ewalds, Timo and Hafner, Roland and Abdolmaleki, Abbas and De Las Casas, Diego and Donner, Craig and Fritz, Leslie and Galperti, Cristian and Huber, Andrea and Keeling, James and Tsimpoukelli, Maria and Kay, Jackie and Merle, Antoine and Moret, Jean-Marc and Noury, Seb and Pesamosca, Federico and Pfau, David and Sauter, Olivier and Sommariva, Cristian and Coda, Stefano and Duval, Basil and Fasoli, Ambrogio and Kohli, Pushmeet and Kavukcuoglu, Koray and Hassabis, Demis and Riedmiller, Martin},
  month    = feb,
  year     = {2022},
  pages    = {414--419}
},
@inproceedings{karl,
  title    = {Koopman-{Assisted} {Reinforcement} {Learning}},
  url      = {https://openreview.net/forum?id=IaUDEYN48p},
  abstract = {The Bellman equation and its continuous form, the Hamilton-Jacobi-Bellman (HJB) equation, are ubiquitous in reinforcement learning and control theory contexts due, in part, to their guaranteed convergence towards a system’s optimal value function. However, its application presents very intense limitations. This paper explores the connection between the data-driven Koopman operator and Bellman Markov Decision Processes, resulting in the development of two new reinforcement learning algorithms to alleviate these limitations. In particular, we focus on Koopman operator methods that reformulate a nonlinear system by lifting into a new coordinate system where the dynamics become linear, and where HJB-based methods are more tractable. These transformations enable the estimation, prediction, and control of strongly nonlinear dynamics. Viewing the Bellman equation as a controlled dynamical system, the Koopman operator is able to describe the expectation of the time evolution of the value function in the given systems via linear dynamics in the lifted coordinates. By parameterizing the Koopman operator with control actions and making an assumption about the feature space of the time evolution of the value function, we are able to construct a new “Koopman tensor” that facilitates the estimation of the optimal value function. Finally, a transformation of Bellman’s framework in terms of the Koopman tensor enables us to reformulate two max-entropy reinforcement learning algorithms: soft-value iteration and soft actor-critic (SAC). This framework is very flexible and can be used for deterministic or stochastic systems as well as for discrete or continuous-time dynamics. We show that these algorithms attain SOTA with respect to traditional neural network-based SAC and linear quadratic regulator baselines while retaining interpretability on 3 controlled dynamical systems: the Lorenz system, the fluid flow past a cylinder, and a double-well potential with non-isotropic stochastic forcing.},
  language = {en},
  urldate  = {2024-03-19},
  author   = {Rozwood, Preston and Mehrez, Edward and Paehler, Ludger and Sun, Wen and Brunton, Steven},
  month    = oct,
  year     = {2023},
  file     = {2024-new-submission:C\:\\Users\\xwx\\Zotero\\storage\\KWPRQ95R\\2403.02290.pdf:application/pdf;on-policy.md:C\:\\Users\\xwx\\Zotero\\storage\\XNYJ4DII\\on-policy.md:text/markdown;ori-PDF:C\:\\Users\\xwx\\Zotero\\storage\\DUAGJYFS\\Rozwood 等 - 2023 - Koopman-Assisted Reinforcement Learning.pdf:application/pdf}
},

@article{koopman_hamiltonian_1931,
  title    = {Hamiltonian {Systems} and {Transformation} in {Hilbert} {Space}},
  volume   = {17},
  issn     = {0027-8424, 1091-6490},
  url      = {https://pnas.org/doi/full/10.1073/pnas.17.5.315},
  doi      = {10.1073/pnas.17.5.315},
  language = {en},
  number   = {5},
  urldate  = {2024-05-01},
  journal  = {Proceedings of the National Academy of Sciences},
  author   = {Koopman, B. O.},
  month    = may,
  year     = {1931},
  pages    = {315--318}
},
@article{koopman_dynamical_1932,
  title    = {Dynamical {Systems} of {Continuous} {Spectra}},
  volume   = {18},
  issn     = {0027-8424, 1091-6490},
  url      = {https://pnas.org/doi/full/10.1073/pnas.18.3.255},
  doi      = {10.1073/pnas.18.3.255},
  language = {en},
  number   = {3},
  urldate  = {2024-05-01},
  journal  = {Proceedings of the National Academy of Sciences},
  author   = {Koopman, B. O. and Neumann, J. V.},
  month    = mar,
  year     = {1932},
  pages    = {255--263}
},
@article{mezic_comparison_2004,
  title     = {Comparison of systems with complex behavior},
  volume    = {197},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  issn      = {01672789},
  url       = {https://linkinghub.elsevier.com/retrieve/pii/S0167278904002507},
  doi       = {10.1016/j.physd.2004.06.015},
  language  = {en},
  number    = {1-2},
  urldate   = {2024-05-01},
  journal   = {Physica D: Nonlinear Phenomena},
  author    = {Mezić, Igor and Banaszuk, Andrzej},
  month     = oct,
  year      = {2004},
  pages     = {101--133}
},
@article{bemporad_explicit_2002,
	title = {The explicit linear quadratic regulator for constrained systems},
	volume = {38},
	issn = {0005-1098},
	url = {https://www.sciencedirect.com/science/article/pii/S0005109801001741},
	doi = {10.1016/S0005-1098(01)00174-1},
	abstract = {For discrete-time linear time invariant systems with constraints on inputs and states, we develop an algorithm to determine explicitly, the state feedback control law which minimizes a quadratic performance criterion. We show that the control law is piece-wise linear and continuous for both the finite horizon problem (model predictive control) and the usual infinite time measure (constrained linear quadratic regulation). Thus, the on-line control computation reduces to the simple evaluation of an explicitly defined piecewise linear function. By computing the inherent underlying controller structure, we also solve the equivalent of the Hamilton–Jacobi–Bellman equation for discrete-time linear constrained systems. Control based on on-line optimization has long been recognized as a superior alternative for constrained systems. The technique proposed in this paper is attractive for a wide range of practical problems where the computational complexity of on-line optimization is prohibitive. It also provides an insight into the structure underlying optimization-based controllers.},
	number = {1},
	urldate = {2024-05-01},
	journal = {Automatica},
	author = {Bemporad, Alberto and Morari, Manfred and Dua, Vivek and Pistikopoulos, Efstratios N.},
	month = jan,
	year = {2002},
	keywords = {Constraints, Linear quadratic regulators, Piecewise linear controllers, Predictive control},
	pages = {3--20},
},
@incollection{fernandez-camacho_robust_1995,
	address = {London},
	title = {Robust {MPC}},
	isbn = {978-1-4471-3008-6},
	url = {https://doi.org/10.1007/978-1-4471-3008-6_7},
	abstract = {Mathematical models of real processes cannot contemplate every aspect of reality. Simplifying assumptions have to be made, especially when the models are going to be used for control purposes, where models with simple structures (linear in most cases) and sufficiently small size have to be used due to available control techniques and real time considerations. Thus, mathematical models, and especially control models, can only describe the dynamics of the process in an approximative way.},
	language = {en},
	urldate = {2024-05-01},
	booktitle = {Model {Predictive} {Control} in the {Process} {Industry}},
	publisher = {Springer},
	author = {Fernandez-Camacho, Eduardo and Bordons-Alba, Camacho Carlos},
	editor = {Fernandez-Camacho, Eduardo and Bordons-Alba, Carlos},
	year = {1995},
	doi = {10.1007/978-1-4471-3008-6_7},
	pages = {155--170},
},
@inproceedings{mainuddin_detecting_2023,
	title = {Detecting {Compromised} {IoT} {Devices} {Using} {Autoencoders} with {Sequential} {Hypothesis} {Testing}},
	url = {https://ieeexplore.ieee.org/document/10386399},
	doi = {10.1109/BigData59044.2023.10386399},
	abstract = {IoT devices fundamentally lack built-in security mechanisms to protect themselves from security attacks. Existing works on improving IoT security mostly focus on detecting anomalous behaviors of IoT devices. However, these existing anomaly detection schemes may trigger an overwhelmingly large number of false alerts, rendering them unusable in detecting compromised IoT devices. In this paper we develop an effective and efficient framework, named CUMAD, to detect compromised IoT devices. Instead of directly relying on individual anomalous events, CUMAD aims to accumulate sufficient evidence in detecting compromised IoT devices, by integrating an autoencoder-based anomaly detection subsystem with a sequential probability ratio test (SPRT)-based sequential hypothesis testing subsystem. CUMAD can effectively reduce the number of false alerts in detecting compromised IoT devices, and moreover, it can detect compromised IoT devices quickly. Our evaluation studies based on the public-domain N-BaIoT dataset show that CUMAD can on average reduce the false positive rate from about 3.57\% using only the autoencoder-based anomaly detection scheme to about 0.5\%; in addition, CUMAD can detect compromised IoT devices quickly, with less than 5 observations on average.},
	urldate = {2024-05-01},
	booktitle = {2023 {IEEE} {International} {Conference} on {Big} {Data} ({BigData})},
	author = {Mainuddin, Md and Duan, Zhenhai and Dong, Yingfei},
	month = dec,
	year = {2023},
	keywords = {Behavioral sciences, Big Data, Internet of Things, Neural networks, Performance evaluation, Probability, Rendering (computer graphics)},
	pages = {1344--1351},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\xwx\\Zotero\\storage\\2PFV4M2B\\10386399.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\xwx\\Zotero\\storage\\TNG3GQ53\\Mainuddin 等 - 2023 - Detecting Compromised IoT Devices Using Autoencode.pdf:application/pdf},
},
@inproceedings{han_deep_2020,
	title = {Deep {Learning} of {Koopman} {Representation} for {Control}},
	url = {https://ieeexplore.ieee.org/document/9304238},
	doi = {10.1109/CDC42340.2020.9304238},
	abstract = {We develop a data-driven, model-free approach for the optimal control of the dynamical system. The proposed approach relies on the Deep Neural Network (DNN) based learning of Koopman operator for the purpose of control. In particular, DNN is employed for the data-driven identification of basis function used in the linear lifting of nonlinear control system dynamics. The controller synthesis is purely data-driven and does not rely on a priori domain knowledge. The OpenAI Gym environment, employed for Reinforcement Learning-based control design, is used for data generation and learning of Koopman operator in control setting. The method is applied to two classic dynamical systems on OpenAI Gym environment to demonstrate the capability.},
	urldate = {2024-05-01},
	booktitle = {2020 59th {IEEE} {Conference} on {Decision} and {Control} ({CDC})},
	author = {Han, Yiqiang and Hao, Wenjian and Vaidya, Umesh},
	month = dec,
	year = {2020},
	note = {ISSN: 2576-2370},
	keywords = {Aerospace electronics, Control systems, Mathematical model, Nonlinear dynamical systems, Reinforcement learning, Training, Vehicle dynamics},
	pages = {1890--1895},
},
@article{dkn,
	title = {Deep {Koopman} {Operator} {With} {Control} for {Nonlinear} {Systems}},
	volume = {7},
	issn = {2377-3766},
	url = {https://ieeexplore.ieee.org/document/9799788},
	doi = {10.1109/LRA.2022.3184036},
	abstract = {Recently Koopman operator has become a promising data-driven tool to facilitate real-time control for unknown nonlinear systems. It maps nonlinear systems into equivalent linear systems in embedding space, ready for real-time linear control methods. However, designing an appropriate Koopman embedding function remains a challenging task. Furthermore, most Koopman-based algorithms only consider nonlinear systems with linear control input, resulting in lousy prediction and control performance when the system is fully nonlinear with the control input. In this work, we propose an end-to-end deep learning framework to learn the Koopman embedding function and Koopman Operator together to alleviate such difficulties. We first parameterize the embedding function and Koopman Operator with the neural network and train them end-to-end with the K-steps loss function. Then, an auxiliary control network is augmented to encode the nonlinear state-dependent control term to model the nonlinearity in the control input. This encoded term is considered the new control variable instead to ensure linearity of the modeled system in the embedding system. We next deploy Linear Quadratic Regulator (LQR) on the linear embedding space to derive the optimal control policy and decode the actual control input from the control net. Experimental results demonstrate that our approach outperforms other existing methods, reducing the prediction error by order of magnitude and achieving superior control performance in several nonlinear dynamic systems like damping pendulum, CartPole, and the seven DOF robotic manipulator.},
	number = {3},
	urldate = {2024-03-19},
	journal = {IEEE Robotics and Automation Letters},
	author = {Shi, Haojie and Meng, Max Q.-H.},
	month = jul,
	year = {2022},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	keywords = {Aerospace electronics, Control systems, Deep learning, Deep learning methods, Linearity, machine learning for robot control, model learning for control, Nonlinear dynamical systems, Optimal control, Real-time systems},
	pages = {7700--7707},
	file = {已提交版本:C\:\\Users\\xwx\\Zotero\\storage\\34EM2347\\Shi和Meng - 2022 - Deep Koopman Operator With Control for Nonlinear Systems.pdf:application/pdf},
},
@misc{noauthor_-_nodate,
	title = {什么是神经网络？- 人工神经网络简介 - {AWS}},
	url = {https://aws.amazon.com/cn/what-is/neural-network/},
	abstract = {了解什么是神经网络，企业如何使用神经网络与使用原因，以及如何在 AWS 上使用神经网络。},
	language = {zh-CN},
	urldate = {2024-05-01},
	journal = {Amazon Web Services, Inc.},
	file = {Snapshot:C\:\\Users\\xwx\\Zotero\\storage\\IH3HL65U\\neural-network.html:text/html},
}

@misc{noauthor__2024,
	title = {人工神经网络},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://zh.wikipedia.org/w/index.php?title=%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&oldid=81838422},
	abstract = {人工神经网络（英语：artificial neural network，ANNs）简称神经网络（neural network，NNs）或类神经网络，在机器学习和认知科学领域，是一种模仿生物神经网络（动物的中枢神经系统，特别是大脑）的结构和功能的数学模型或计算模型，用于对函数进行估计或近似。神经网络由大量的人工神经元联结进行计算。大多数情况下人工神经网络能在外界信息的基础上改变内部结构，是一种自适应系统，通俗地讲就是具备学习功能。现代神经网络是一种非线性统计性数据建模工具，神经网络通常是通过一个基于数学统计学类型的学习方法（learning method）得以优化，所以也是数学统计学方法的一种实际应用，通过统计学的标准数学方法我们能够得到大量的可以用函数来表达的局部结构空间，另一方面在人工智能学的人工感知领域，我们通过数学统计学的应用可以来做人工感知方面的决定问题（也就是说通过统计学的方法，人工神经网络能够类似人一样具有简单的决定能力和简单的判断能力），这种方法比起正式的逻辑学推理演算更具有优势。
和其他机器学习方法一样，神经网络已经被用于解决各种各样的问题，例如机器视觉和语音识别。这些问题都是很难被传统基于规则的编程所解决的。},
	language = {zh-Hans-CN},
	urldate = {2024-05-01},
	journal = {维基百科，自由的百科全书},
	month = mar,
	year = {2024},
	note = {Page Version ID: 81838422},
},
@article{schmid_dynamic_2010,
	title = {Dynamic mode decomposition of numerical and experimental data},
	volume = {656},
	issn = {1469-7645, 0022-1120},
	url = {https://www.cambridge.org/core/journals/journal-of-fluid-mechanics/article/dynamic-mode-decomposition-of-numerical-and-experimental-data/AA4C763B525515AD4521A6CC5E10DBD4#},
	doi = {10.1017/S0022112010001217},
	abstract = {The description of coherent features of fluid flow is essential to our understanding of fluid-dynamical and transport processes. A method is introduced that is able to extract dynamic information from flow fields that are either generated by a (direct) numerical simulation or visualized/measured in a physical experiment. The extracted dynamic modes, which can be interpreted as a generalization of global stability modes, can be used to describe the underlying physical mechanisms captured in the data sequence or to project large-scale problems onto a dynamical system of significantly fewer degrees of freedom. The concentration on subdomains of the flow field where relevant dynamics is expected allows the dissection of a complex flow into regions of localized instability phenomena and further illustrates the flexibility of the method, as does the description of the dynamics within a spatial framework. Demonstrations of the method are presented consisting of a plane channel flow, flow over a two-dimensional cavity, wake flow behind a flexible membrane and a jet passing between two cylinders.},
	language = {en},
	urldate = {2024-05-01},
	journal = {Journal of Fluid Mechanics},
	author = {Schmid, Peter J.},
	month = aug,
	year = {2010},
	pages = {5--28},
},
@article{williams_data-driven_2015,
	title = {A {Data}-{Driven} {Approximation} of the {Koopman} {Operator}: {Extending} {Dynamic} {Mode} {Decomposition}},
	volume = {25},
	issn = {0938-8974, 1432-1467},
	shorttitle = {A {Data}-{Driven} {Approximation} of the {Koopman} {Operator}},
	url = {http://arxiv.org/abs/1408.4408},
	doi = {10.1007/s00332-015-9258-5},
	abstract = {The Koopman operator is a linear but infinite dimensional operator that governs the evolution of scalar observables defined on the state space of an autonomous dynamical system, and is a powerful tool for the analysis and decomposition of nonlinear dynamical systems. In this manuscript, we present a data driven method for approximating the leading eigenvalues, eigenfunctions, and modes of the Koopman operator. The method requires a data set of snapshot pairs and a dictionary of scalar observables, but does not require explicit governing equations or interaction with a "black box" integrator. We will show that this approach is, in effect, an extension of Dynamic Mode Decomposition (DMD), which has been used to approximate the Koopman eigenvalues and modes. Furthermore, if the data provided to the method are generated by a Markov process instead of a deterministic dynamical system, the algorithm approximates the eigenfunctions of the Kolmogorov backward equation, which could be considered as the "stochastic Koopman operator" [1]. Finally, four illustrative examples are presented: two that highlight the quantitative performance of the method when presented with either deterministic or stochastic data, and two that show potential applications of the Koopman eigenfunctions.},
	number = {6},
	urldate = {2024-05-01},
	journal = {Journal of Nonlinear Science},
	author = {Williams, Matthew O. and Kevrekidis, Ioannis G. and Rowley, Clarence W.},
	month = dec,
	year = {2015},
	note = {arXiv:1408.4408 [math]},
	keywords = {Mathematics - Dynamical Systems},
	pages = {1307--1346},
	file = {arXiv Fulltext PDF:C\:\\Users\\xwx\\Zotero\\storage\\D9JE76HD\\Williams 等 - 2015 - A Data-Driven Approximation of the Koopman Operato.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\xwx\\Zotero\\storage\\J5EXQSSW\\1408.html:text/html},
},
@article{brunton_sparse_2016,
	series = {10th {IFAC} {Symposium} on {Nonlinear} {Control} {Systems} {NOLCOS} 2016},
	title = {Sparse {Identification} of {Nonlinear} {Dynamics} with {Control} ({SINDYc})*},
	volume = {49},
	issn = {2405-8963},
	url = {https://www.sciencedirect.com/science/article/pii/S2405896316318298},
	doi = {10.1016/j.ifacol.2016.10.249},
	abstract = {Identifying governing equations from data is a critical step in the modeling and control of complex dynamical systems. Here, we investigate the data-driven identification of nonlinear dynamical systems with inputs and forcing using regression methods, including sparse regression. Specifically, we generalize the sparse identification of nonlinear dynamics (SINDY) algorithm to include external inputs and feedback control. This method is demonstrated on examples including the Lotka-Volterra predator-prey model and the Lorenz system with forcing and control. We also connect the present algorithm with the dynamic mode decomposition (DMD) and Koopman operator theory to provide a broader context.},
	number = {18},
	urldate = {2024-05-01},
	journal = {IFAC-PapersOnLine},
	author = {Brunton, Steven L. and Proctor, Joshua L. and Kutz, J. Nathan},
	month = jan,
	year = {2016},
	keywords = {control, Dynamical systems, sparse regression, system identification},
	pages = {710--715},
},
@inproceedings{haarnoja_reinforcement_2017,
	address = {Sydney, NSW, Australia},
	series = {{ICML}'17},
	title = {Reinforcement learning with deep energy-based policies},
	abstract = {We propose a method for learning expressive energy-based policies for continuous states and actions, which has been feasible only in tabular domains before. We apply our method to learning maximum entropy policies, resulting into a new algorithm, called soft Q-learning, that expresses the optimal policy via a Boltzmann distribution. We use the recently proposed amortized Stein variational gradient descent to learn a stochastic sampling network that approximates samples from this distribution. The benefits of the proposed algorithm include improved exploration and compositionality that allows transferring skills between tasks, which we confirm in simulated experiments with swimming and walking robots. We also draw a connection to actor-critic methods, which can be viewed performing approximate inference on the corresponding energy-based model.},
	urldate = {2024-05-01},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning} - {Volume} 70},
	publisher = {JMLR.org},
	author = {Haarnoja, Tuomas and Tang, Haoran and Abbeel, Pieter and Levine, Sergey},
	month = aug,
	year = {2017},
	pages = {1352--1361},
},
@article{haarnoja_soft_2018,
	title = {Soft {Actor}-{Critic}: {Off}-{Policy} {Maximum} {Entropy} {Deep} {Reinforcement} {Learning} with a {Stochastic} {Actor}},
	volume = {abs/1801.01290},
	shorttitle = {Soft {Actor}-{Critic}},
	url = {http://arxiv.org/abs/1801.01290},
	urldate = {2024-05-01},
	journal = {CoRR},
	author = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
	year = {2018},
	note = {arXiv: 1801.01290},
},
@article{sharma_introduction_1998,
	title = {Introduction of {Reinforcement} {Learning} and {Its} {Application} {Across} {Different} {Domain}},
	volume = {9},
	issn = {2456-3307},
	url = {https://res.ijsrcseit.com/CSEIT239066},
	doi = {10.32628/CSEIT239066},
	language = {en},
	number = {6},
	urldate = {2024-05-01},
	journal = {International Journal of Scientific Research in Computer Science, Engineering and Information Technology},
	author = {Sharma, Harshita and Kumar, Hritik and Pandey, Rashmi},
	year = {1998},
	pages = {98--104},
},
@article{brunton_chaos_2017,
	title = {Chaos as an intermittently forced linear system},
	volume = {8},
	copyright = {2017 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-017-00030-8},
	doi = {10.1038/s41467-017-00030-8},
	abstract = {Understanding the interplay of order and disorder in chaos is a central challenge in modern quantitative science. Approximate linear representations of nonlinear dynamics have long been sought, driving considerable interest in Koopman theory. We present a universal, data-driven decomposition of chaos as an intermittently forced linear system. This work combines delay embedding and Koopman theory to decompose chaotic dynamics into a linear model in the leading delay coordinates with forcing by low-energy delay coordinates; this is called the Hankel alternative view of Koopman (HAVOK) analysis. This analysis is applied to the Lorenz system and real-world examples including Earth’s magnetic field reversal and measles outbreaks. In each case, forcing statistics are non-Gaussian, with long tails corresponding to rare intermittent forcing that precedes switching and bursting phenomena. The forcing activity demarcates coherent phase space regions where the dynamics are approximately linear from those that are strongly nonlinear.},
	language = {en},
	number = {1},
	urldate = {2024-05-01},
	journal = {Nature Communications},
	author = {Brunton, Steven L. and Brunton, Bingni W. and Proctor, Joshua L. and Kaiser, Eurika and Kutz, J. Nathan},
	month = may,
	year = {2017},
	note = {Publisher: Nature Publishing Group},
	keywords = {Applied mathematics, Computational science, Scientific data},
	pages = {19},
	file = {Full Text PDF:C\:\\Users\\xwx\\Zotero\\storage\\I9598HUV\\Brunton 等 - 2017 - Chaos as an intermittently forced linear system.pdf:application/pdf},
},
@article{huang2022cleanrl,
  author  = {Shengyi Huang and Rousslan Fernand Julien Dossa and Chang Ye and Jeff Braga and Dipam Chakraborty and Kinal Mehta and João G.M. Araújo},
  title   = {CleanRL: High-quality Single-file Implementations of Deep Reinforcement Learning Algorithms},
  journal = {Journal of Machine Learning Research},
  year    = {2022},
  volume  = {23},
  number  = {274},
  pages   = {1--18},
  url     = {http://jmlr.org/papers/v23/21-1342.html}
},